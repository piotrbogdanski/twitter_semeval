{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semeval 2017 Task - Comparison of Different Classification Traditional and Neural Classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to compare the performance of traditional machine learning models using the bag-of-words text data representation with modern neural network models making use of advanced vectors space models (GloVe) and state-of-the-art transformer methods (BERT). The following models were compared:\n",
    "\n",
    "**BoW classifiers - using count and TfIdf representations**:\n",
    "1. Naive Bayes Classifier\n",
    "2. Logistic Regression/Maximum Entropy Classifier\n",
    "3. Linear Kernel Support Vector Classifier\n",
    "4. K Nearest Neighbours Classifier\n",
    "\n",
    "**Nerual Networks**:\n",
    "1. Uni-Directional LSTM using pre-trained GloVe Embeddings\n",
    "2. Bi-Directional LSTM using pre-trained GloVe Embeddings\n",
    "3. Pre-trained BERT transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tools:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#text:\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "#preprocessing:\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I load the data and evaluate the class distribution in each of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_headers = ['id','sentiment','text']\n",
    "twitter_train = pd.read_csv('data/twitter-training-data.txt',sep = '\\t',names = twitter_data_headers)\n",
    "twitter_dev = pd.read_csv('data/twitter-dev-data.txt',sep = '\\t',names = twitter_data_headers)\n",
    "twitter_test1 = pd.read_csv('data/twitter-test1.txt',sep = '\\t',names = twitter_data_headers)\n",
    "twitter_test2 = pd.read_csv('data/twitter-test2.txt',sep = '\\t',names = twitter_data_headers)\n",
    "twitter_test3 = pd.read_csv('data/twitter-test3.txt',sep = '\\t',names = twitter_data_headers)\n",
    "twitter_train['split'] = 'train'\n",
    "twitter_dev['split'] = 'dev'\n",
    "twitter_test1['split'] = 'test1'\n",
    "twitter_test2['split'] = 'test2'\n",
    "twitter_test3['split'] = 'test3'\n",
    "twitter_data = twitter_train.append([twitter_dev,twitter_test1,twitter_test2,twitter_test2],ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution\n",
    "Overall, neutral sentiment dominates in the training data, however the development and validation corpuses are more balanced between the positive and the neutral class. Each of the sets has the least tweets with the negative sentiment, which suggests that recognizing this class will be most difficult and also important. A good metric that should provide an overview of the classifier performance is the micro-averaged F1 Score. This metric is a harmonic mean of precision and recall, in which the contribution of each class is the same, regardless of the data imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "barchart = twitter_data.groupby(\"split\").sentiment.value_counts().unstack().plot.barh(figsize = (14, 10))\n",
    "barchart.set_xlabel('Number of Tweets')\n",
    "barchart.set_ylabel('Dataset')\n",
    "fig = barchart.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Bag-of-Words and Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features \n",
    "For the bow models, it may be important to design explicit features from the data that may be indicative of a partiuclar class and will help discriminate between sentiments. Such characteristic features include emoticons, emojis, user mentions, hashstags, url and numbers in the tweets. The table below summarizes it for the training and development datasets, to evaluate whether they should simply be dropped in preprocessing or replaced with a special token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(tweets):\n",
    "    \n",
    "    #count URls\n",
    "    re_url = re.compile(r'(?:https?\\:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.(?:(?:[a-zA-Z0-9-]+\\.)*)?[a-z]{2,4}(?:(?:\\/\\S+)*)?')\n",
    "    urls = [re_url.findall(x) for x in tweets]\n",
    "    urls_count = [len(x) for x in urls]\n",
    "    \n",
    "    #remove urls\n",
    "    tweets = [tweet.lower() for tweet in tweets] #lowercase\n",
    "    tweets = [re.sub(r'(?:https?\\:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.(?:(?:[a-zA-Z0-9-]+\\.)*)?[a-z]{2,4}(?:(?:\\/\\S+)*)?',' ',tweet) for tweet in tweets]\n",
    "    #remove extra whitespaces:\n",
    "    tweets = [re.sub(r' +',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #get counts of emojis and emoticons\n",
    "    re_emoji = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "    emojis = [re_emoji.findall(x) for x in tweets]\n",
    "    emoji_count = [len(x) for x in emojis]\n",
    "    \n",
    "    re_emoticons_pos = re.compile(r\"[oO>]?[;:=Xx8]+'?\\-?[)>)pPdD3\\]\\}\\*]+\")\n",
    "    emoticons_pos = [re_emoticons_pos.findall(x) for x in tweets]\n",
    "    emoticon_count_pos = [len(x) for x in emoticons_pos]\n",
    "    \n",
    "    re_emoticons_neg = re.compile(r\"[oO>]?[:;=]+'?\\-?[(<\\\\\\/\\[(\\{]+\")\n",
    "    emoticons_neg = [re_emoticons_neg.findall(x) for x in tweets]\n",
    "    emoticon_count_neg = [len(x) for x in emoticons_neg]\n",
    "\n",
    "    re_emoticons_neut = re.compile(r\"[oO>]?[:;=]+'?\\-?[oO]+\")\n",
    "    emoticons_neut = [re_emoticons_neut.findall(x) for x in tweets]\n",
    "    emoticon_count_neut = [len(x) for x in emoticons_neut]\n",
    "    \n",
    "    #get hashtags counts\n",
    "    re_hashtag = re.compile(r'\\#[a-zA-Z0-9]+')\n",
    "    hashtags = [re_hashtag.findall(x) for x in tweets]\n",
    "    hashtag_count = [len(x) for x in hashtags]\n",
    "    \n",
    "    #get mention counts\n",
    "    re_mention = re.compile(r'\\@\\w+(?!\\.\\w+)\\b')\n",
    "    mentions = [re_mention.findall(x) for x in tweets]\n",
    "    mention_count = [len(x) for x in mentions]\n",
    "    \n",
    "    #get number counts\n",
    "    re_number = re.compile(r\"(\\S+)?\\d+(\\S+)?\")\n",
    "    numbers = [re_number.findall(x) for x in tweets]\n",
    "    number_count = [len(x) for x in numbers]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(list(zip(urls_count,emoji_count,emoticon_count_pos,\n",
    "                                 emoticon_count_neg,emoticon_count_neut,hashtag_count,mention_count,number_count)),\n",
    "                        columns = ['urls','emojis','emoticons-positive',\n",
    "                                   'emoticons-negative','emoticons-neutral',\n",
    "                                   'hashtags','mentions','numbers'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urls</th>\n",
       "      <td>0.289167</td>\n",
       "      <td>0.409803</td>\n",
       "      <td>0.312469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emojis</th>\n",
       "      <td>0.024528</td>\n",
       "      <td>0.014844</td>\n",
       "      <td>0.038910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emoticons-positive</th>\n",
       "      <td>0.029578</td>\n",
       "      <td>0.041062</td>\n",
       "      <td>0.087719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emoticons-negative</th>\n",
       "      <td>0.022484</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.002757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emoticons-neutral</th>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.001003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtags</th>\n",
       "      <td>0.364074</td>\n",
       "      <td>0.433226</td>\n",
       "      <td>0.384211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentions</th>\n",
       "      <td>0.528796</td>\n",
       "      <td>0.427539</td>\n",
       "      <td>0.405639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numbers</th>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.753145</td>\n",
       "      <td>0.672180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment           negative   neutral  positive\n",
       "urls                0.289167  0.409803  0.312469\n",
       "emojis              0.024528  0.014844  0.038910\n",
       "emoticons-positive  0.029578  0.041062  0.087719\n",
       "emoticons-negative  0.022484  0.005253  0.002757\n",
       "emoticons-neutral   0.001082  0.000530  0.001003\n",
       "hashtags            0.364074  0.433226  0.384211\n",
       "mentions            0.528796  0.427539  0.405639\n",
       "numbers             0.489600  0.753145  0.672180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = count_features(twitter_train[\"text\"].tolist())\n",
    "test_counts = test_counts.groupby(twitter_train[\"sentiment\"]).mean().transpose()\n",
    "test_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>urls</th>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.425462</td>\n",
       "      <td>0.330014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emojis</th>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.071124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emoticons-positive</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.044614</td>\n",
       "      <td>0.102418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emoticons-negative</th>\n",
       "      <td>0.010582</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.004267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emoticons-neutral</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtags</th>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.499456</td>\n",
       "      <td>0.372688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentions</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.428727</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numbers</th>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.737758</td>\n",
       "      <td>0.694168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment           negative   neutral  positive\n",
       "urls                0.296296  0.425462  0.330014\n",
       "emojis              0.007937  0.016322  0.071124\n",
       "emoticons-positive  0.023810  0.044614  0.102418\n",
       "emoticons-negative  0.010582  0.008705  0.004267\n",
       "emoticons-neutral   0.000000  0.002176  0.002845\n",
       "hashtags            0.365079  0.499456  0.372688\n",
       "mentions            0.555556  0.428727  0.421053\n",
       "numbers             0.513228  0.737758  0.694168"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_counts = count_features(twitter_dev[\"text\"].tolist())\n",
    "dev_counts = dev_counts.groupby(twitter_dev[\"sentiment\"]).mean().transpose()\n",
    "dev_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis suggests that **positive and negative emojis** are useful features, as they correspond more ofthen with their respective class occurences. **User mentions** are expected to discriminate the negative class particularly well, therefore they will be included as explicit features too. **Numbers** also appear to be used more often by positive and neutral than negative tweets. The rest of the features will be dropped in preprocessing, as they don't seem to discriminate well between the negative amnd the positive class, which is most important in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for the bag-of-words model\n",
    "Taking into account the above analysis, I use regualar expressions to preprocess the data, by:\n",
    "1. Lowercasing the data\n",
    "2. Replacing user mentions, numbers and positive/negative emoticons with special tokens.\n",
    "3. Removing URLs, emojis and hashtags.\n",
    "4. Removing words of length 1\n",
    "5. Removing non-alphanumeric data\n",
    "6. Performing **tokenizetion** using `nltk.word_tokenize`\n",
    "7. Performing **stopword removal** using `nltk`'s English stopwords, with the exception of negation words, which might be useful in the bag of words bigram model.\n",
    "8. Performing **lemmatization** using `nltk`'s word-net lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bow(tweets):\n",
    "    tweets = [tweet.lower() for tweet in tweets] #lowercase\n",
    "    \n",
    "    #remove urls\n",
    "    tweets = [re.sub(r\"(?:https?\\:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.(?:(?:[a-zA-Z0-9-]+\\.)*)?[a-z]{2,4}(?:(?:(\\/|\\?)\\S+)*)?\",' ',tweet) for tweet in tweets]\n",
    "\n",
    "    #remove emojis\n",
    "    tweets = [re.sub(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #replace emoticons with a token\n",
    "    tweets = [re.sub(r\"[oO>]?[;:=Xx8]+'?\\-?[)>)pPdD3\\]\\}\\*]+\",' emoticonpos ',tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r\"[oO>]?[:;=]+'?\\-?[(<\\\\\\/\\[)\\{]+\",' emoticonneg ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove words of length == 1\n",
    "    tweets = [re.sub(r'\\b(\\w)\\b',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #replace numbers with a token\n",
    "    tweets = [re.sub(r\"(\\S+)?\\d+(\\S+)?\",' numbertoken ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove hashtags\n",
    "    tweets = [re.sub(r'\\#[a-zA-Z0-9]+',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #replace user mentions with a token\n",
    "    tweets = [re.sub(r'\\@\\w+(?!\\.\\w+)\\b',' mentiontoken ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove non alphanumeric\n",
    "    tweets = [re.sub(r\"\\&amp\",\" \",tweet) for tweet in tweets] #bug in twitter - &amp appearing\n",
    "    tweets = [re.sub(r\"[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]+\",\" \",tweet) for tweet in tweets]\n",
    "    #replace underscores with space\n",
    "    tweets = [re.sub(r\"_+\",\" \",tweet) for tweet in tweets]\n",
    "    \n",
    "    \n",
    "    #remove extra whitespaces:\n",
    "    tweets = [re.sub(r' +',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    \n",
    "    #perform tokenzation:\n",
    "    tweets = [word_tokenize(word) for word in tweets]\n",
    "    \n",
    "    #perform stopword removal - except for negation words\n",
    "    stop_words = {word for word in stopwords.words('english') if not re.compile(r\"\\b(\\w+nt|no|not)\\b\").match(word)}\n",
    "    for i in trange(len(tweets)):\n",
    "        tweets[i] = [word for word in tweets[i] if word not in stop_words]\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45026/45026 [00:00<00:00, 390148.07it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 289122.77it/s]\n",
      "100%|██████████| 3531/3531 [00:00<00:00, 359171.74it/s]\n",
      "100%|██████████| 1853/1853 [00:00<00:00, 252478.49it/s]\n",
      "100%|██████████| 2379/2379 [00:00<00:00, 243996.80it/s]\n"
     ]
    }
   ],
   "source": [
    "train, dev, test1, test2, test3 = list(map(lambda x: preprocess_bow(x[\"text\"].tolist()), \n",
    "                                           [twitter_train, twitter_dev, twitter_test1, twitter_test2, twitter_test3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization:\n",
    "Perform text lemmatization using WordNet lemmatizer combined with nltk's pretrained PoS tagger. In case of no tag existing for a token, treat it as a noun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wordnet_pos(tag):\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ #adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB #verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN #noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV #adverb\n",
    "    else:\n",
    "        return wordnet.NOUN #noun if doesn't match anything\n",
    "    \n",
    "def lemmatize(tweets):\n",
    "    #obtain treebank tags using nltk's pretrained pos tagger\n",
    "    treebank_tagged = [pos_tag(tweet) for tweet in tweets]\n",
    "    \n",
    "    #convert these to wordnet pos tags\n",
    "    wordnet_tagged = [list(map(lambda x: (x[0], wordnet_pos(x[1])), tags)) for tags in treebank_tagged]\n",
    "    \n",
    "    \n",
    "    #lemmatize the tweets\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tweets = [list(map(lambda x: lemmatizer.lemmatize(x[0], x[1]), tagged)) for tagged in wordnet_tagged]\n",
    "    \n",
    "    #join back into strings\n",
    "    tweets = [\" \".join(tweet) for tweet in tweets] #join back into strings\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test1, test2, test3 = list(map(lambda x: lemmatize(x), \n",
    "                                           [train, dev, test1, test2, test3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual evaluation of the preprocessing results\n",
    "As a sanity check, I evaluate the distribution of the preprocessed corpus, to like whether no redundant or unexpected words show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq = FreqDist(list(chain.from_iterable([tweet.split() for tweet in train])))\n",
    "most_common = freq.most_common(20)\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.barh(y = [word[0] for word in most_common], width = [word[1] for word in most_common])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also obtain the most popular positive words that didn't feature in most popular negative vocabulary and vice-versa, to see whether the pre-processed vocabulary discriminates well between the key classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pos = FreqDist(list(chain.from_iterable([tweet.split() \n",
    "                                          for i, tweet in enumerate(train) \n",
    "                                          if twitter_train[\"sentiment\"][i] == \"positive\"])))\n",
    "freq_neg = FreqDist(list(chain.from_iterable([tweet.split() \n",
    "                                          for i, tweet in enumerate(train) \n",
    "                                          if twitter_train[\"sentiment\"][i] == \"negative\"])))\n",
    "pos_words = [word for word in freq_pos.most_common(100) if  word[0] not in [elem[0] for elem in freq_neg.most_common(100)]]\n",
    "neg_words = [word for word in freq_neg.most_common(100) if  word[0] not in [elem[0] for elem in freq_pos.most_common(100)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (16, 8))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.set_title(\"Most common words in negative, but not in positive\")\n",
    "ax.barh(y = [word[0] for word in neg_words], width = [word[1] for word in neg_words])\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"Most common words in positive, but not in negative\")\n",
    "ax.barh(y = [word[0] for word in pos_words], width = [word[1] for word in pos_words])\n",
    "plt.savefig(\"figures/preprocessing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the fact that the positive tweets include words such as \"happy\", \"great\", \"best\" or \"love\" in the top 100 and negative tweets don't, as well as that negative include words such as \"fuck\", \"people\", \"bad\", \"trump\" and the negative emoticon token, it appears that the preprocessed tokens may actually discriminate fairly well between the positive and negative classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "For each model I performed Grid Search based hyperparameter optimization. The function defined below performs grid search over a specified parameter space for both the vectorizer and the classifier, calculating accuracy and F1 score for each of the parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_dev(Xtrain, Ytrain, Xdev, Ydev, classifier, vectorizer, param_space, return_best = False):\n",
    "    \"\"\"\n",
    "    Run grid search with specified training and development data for \n",
    "    a defined vectorization method and classification model over a parameter space\n",
    "    \n",
    "    Parameters:\n",
    "    Xtrain: array-like\n",
    "    Ytrain: array-like\n",
    "    Xdev: array-like\n",
    "    Ydev: array-like\n",
    "    classifier: sklearn uninitialized class\n",
    "    vectorizer: sklearn.preprocessing.text vectorizer\n",
    "    param_space - dictionary of parameter search space\n",
    "    return_best - boolean, whether to return model with best F1\n",
    "    \n",
    "    Returns:\n",
    "    List of dictionaries containing parameter combinations and f1 scores + accuracy\n",
    "    Best fit pipe, if return_best = True\n",
    "    \"\"\"\n",
    "    keys = param_space.keys() #all parameter names\n",
    "    combs = itertools.product(*param_space.values()) #all combinations of parameters\n",
    "    params = [dict(zip(keys, elem)) for elem in combs] #zip these toegether into a list of dicts\n",
    "    \n",
    "    for i, pars in enumerate(params): #iterate over parameter search space\n",
    "        \n",
    "        #get vectorizer paremeters:\n",
    "        pars_vect = {k:v for k, v in pars.items() if k in inspect.getfullargspec(vectorizer).args}\n",
    "        \n",
    "        #get classifier parameters:\n",
    "        pars_clf = {k:v for k, v in pars.items() if k in inspect.getfullargspec(classifier).args}\n",
    "        \n",
    "        #combine into a pipeline:\n",
    "        pipe = Pipeline([(\"vectorizer\", vectorizer(**pars_vect)), (\"classifier\", classifier(**pars_clf))])\n",
    "        \n",
    "        #fit and validate the pipeline:\n",
    "        try:\n",
    "            pipe.fit(Xtrain, Ytrain)\n",
    "            ypred = pipe.predict(Xdev)\n",
    "            params[i][\"accuracy\"] = accuracy_score(ypred, Ydev)\n",
    "            params[i][\"f1\"] = f1_score(ypred, Ydev, average = \"macro\")\n",
    "        except:\n",
    "            params[i][\"accuracy\"] = np.nan\n",
    "            params[i][\"f1\"] = np.nan\n",
    "            \n",
    "        print(f\"\\rEvaluated parameters {pars}, combination {i+1} out of {len(params)}\", end = \"\")\n",
    "        \n",
    "    if return_best: #return best fit model\n",
    "        f1 = np.array([elem[\"f1\"] for elem in params]) #get all f1 scores\n",
    "        pars = params[np.argmax(f1)] #get parameter combination with highest f1\n",
    "        pars_vect = {k:v for k, v in pars.items() if k in inspect.getfullargspec(vectorizer).args}\n",
    "        pars_clf = {k:v for k, v in pars.items() if k in inspect.getfullargspec(classifier).args}\n",
    "        pipe = Pipeline([(\"vectorizer\", vectorizer(**pars_vect)), (\"classifier\", classifier(**pars_clf))])\n",
    "        pipe.fit(Xtrain, Ytrain)\n",
    "        return params, pipe\n",
    "        \n",
    "    else:\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the tragets into numeric representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylookup = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
    "ytrain, ydev, ytest1, ytest2, ytest3 = [np.array(list(map(lambda y: ylookup[y], y[\"sentiment\"].tolist()))) for y in \n",
    "                                            [twitter_train, twitter_dev, twitter_test1, twitter_test2, twitter_test3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\"alpha\":[1e-10, 1.0, 5.0, 10.0], \n",
    "              \"fit_prior\":[True, False], \n",
    "              \"ngram_range\":[(1,1),(1,2),(2,2)],\n",
    "              \"min_df\":[5, 10, 50]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Count Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'alpha': 10.0, 'fit_prior': False, 'ngram_range': (2, 2), 'min_df': 50, 'accuracy': 0.404, 'f1': 0.40555477866413164}, combination 72 out of 722"
     ]
    }
   ],
   "source": [
    "res_nb_c, pipe_nb_c = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = MultinomialNB, vectorizer = CountVectorizer, \n",
    "                      param_space = param_space, return_best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>fit_prior</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.609570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6150</td>\n",
       "      <td>0.607033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6165</td>\n",
       "      <td>0.606668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.605075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.604848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  fit_prior ngram_range  min_df  accuracy        f1\n",
       "21    1.0       True      (1, 2)       5    0.6115  0.609570\n",
       "40    5.0       True      (1, 2)      10    0.6150  0.607033\n",
       "37    5.0       True      (1, 1)      10    0.6165  0.606668\n",
       "41    5.0       True      (1, 2)      50    0.6125  0.605075\n",
       "48    5.0      False      (1, 2)       5    0.6095  0.604848"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_nb_c).dropna().sort_values(\"f1\",ascending = False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tfidf Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'alpha': 10.0, 'fit_prior': False, 'ngram_range': (2, 2), 'min_df': 50, 'accuracy': 0.414, 'f1': 0.415773087207139}, combination 72 out of 727272"
     ]
    }
   ],
   "source": [
    "res_nb_t, pipe_nb_t = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = MultinomialNB, vectorizer = TfidfVectorizer, \n",
    "                      param_space = param_space, return_best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>fit_prior</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6020</td>\n",
       "      <td>0.600444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6055</td>\n",
       "      <td>0.596974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6090</td>\n",
       "      <td>0.594538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6075</td>\n",
       "      <td>0.591944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>0.591085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  fit_prior ngram_range  min_df  accuracy        f1\n",
       "30    1.0      False      (1, 2)       5    0.6020  0.600444\n",
       "49    5.0      False      (1, 2)      10    0.6055  0.596974\n",
       "45    5.0      False      (1, 1)       5    0.6090  0.594538\n",
       "64   10.0      False      (1, 1)      10    0.6075  0.591944\n",
       "28    1.0      False      (1, 1)      10    0.5925  0.591085"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_nb_t).dropna().sort_values(\"f1\",ascending = False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\"max_iter\":[10000],\n",
    "              \"C\":[10**i for i in range(-3,4)], \n",
    "              \"fit_intercept\":[True, False], \n",
    "              \"ngram_range\":[(1,1),(1,2),(2,2)],\n",
    "              \"min_df\":[5, 10, 50]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Count Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'max_iter': 10000, 'C': 1000, 'fit_intercept': False, 'ngram_range': (2, 2), 'min_df': 50, 'accuracy': 0.519, 'f1': 0.44402973366593684}, combination 126 out of 126"
     ]
    }
   ],
   "source": [
    "res_logit_c, pipe_logit_c = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = MultinomialNB, vectorizer = CountVectorizer, \n",
    "                      param_space = param_space, return_best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_iter</th>\n",
       "      <th>C</th>\n",
       "      <th>fit_intercept</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>10000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>10000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>10000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>10000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>10000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>10000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>10000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6115</td>\n",
       "      <td>0.60957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_iter         C  fit_intercept ngram_range  min_df  accuracy       f1\n",
       "93      10000   100.000           True      (1, 2)       5    0.6115  0.60957\n",
       "39      10000     0.100           True      (1, 2)       5    0.6115  0.60957\n",
       "102     10000   100.000          False      (1, 2)       5    0.6115  0.60957\n",
       "84      10000    10.000          False      (1, 2)       5    0.6115  0.60957\n",
       "30      10000     0.010          False      (1, 2)       5    0.6115  0.60957\n",
       "75      10000    10.000           True      (1, 2)       5    0.6115  0.60957\n",
       "12      10000     0.001          False      (1, 2)       5    0.6115  0.60957\n",
       "111     10000  1000.000           True      (1, 2)       5    0.6115  0.60957\n",
       "120     10000  1000.000          False      (1, 2)       5    0.6115  0.60957\n",
       "66      10000     1.000          False      (1, 2)       5    0.6115  0.60957"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_logit_c).dropna().sort_values(\"f1\",ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tfidf Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'max_iter': 10000, 'C': 1000, 'fit_intercept': False, 'ngram_range': (2, 2), 'min_df': 50, 'accuracy': 0.523, 'f1': 0.43148016096144914}, combination 126 out of 126"
     ]
    }
   ],
   "source": [
    "res_logit_t, pipe_logit_t = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = MultinomialNB, vectorizer = TfidfVectorizer, \n",
    "                      param_space = param_space, return_best = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_iter</th>\n",
       "      <th>C</th>\n",
       "      <th>fit_intercept</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>10000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>10000</td>\n",
       "      <td>1000.000</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>10000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>10000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>10000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.571136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_iter         C  fit_intercept ngram_range  min_df  accuracy        f1\n",
       "31      10000     0.010          False      (1, 2)      10     0.613  0.571136\n",
       "49      10000     0.100          False      (1, 2)      10     0.613  0.571136\n",
       "94      10000   100.000           True      (1, 2)      10     0.613  0.571136\n",
       "112     10000  1000.000           True      (1, 2)      10     0.613  0.571136\n",
       "40      10000     0.100           True      (1, 2)      10     0.613  0.571136\n",
       "13      10000     0.001          False      (1, 2)      10     0.613  0.571136\n",
       "85      10000    10.000          False      (1, 2)      10     0.613  0.571136\n",
       "103     10000   100.000          False      (1, 2)      10     0.613  0.571136\n",
       "22      10000     0.010           True      (1, 2)      10     0.613  0.571136\n",
       "76      10000    10.000           True      (1, 2)      10     0.613  0.571136"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_logit_t).dropna().sort_values(\"f1\",ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Support Vector Machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\"max_iter\":[10000],\n",
    "              \"ngram_range\":[(1,1),(1,2),(2,2)],\n",
    "              \"min_df\":[5, 10],\n",
    "              \"C\":[10**i for i in range(-2,1)],\n",
    "              \"penalty\":[\"l1\",\"l2\"],\n",
    "              \"multi_class\":[\"ovr\",\"crammer\"],\n",
    "              \"fit_itercept\":[True, False],\n",
    "              \"class_weight\":[None, \"balanced\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Count Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'max_iter': 10000, 'ngram_range': (2, 2), 'min_df': 10, 'C': 1, 'penalty': 'l2', 'multi_class': 'crammer', 'fit_itercept': False, 'class_weight': 'balanced', 'accuracy': nan, 'f1': nan}, combination 288 out of 288284 out of 28888"
     ]
    }
   ],
   "source": [
    "res_svc_c, pipe_svc_c = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = LinearSVC, vectorizer = CountVectorizer, \n",
    "                      param_space = param_space, return_best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_iter</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>multi_class</th>\n",
       "      <th>fit_itercept</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6695</td>\n",
       "      <td>0.662028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6695</td>\n",
       "      <td>0.662028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.659995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>0.659995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>0.652575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>0.652575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>0.652007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>0.652007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6455</td>\n",
       "      <td>0.638364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6455</td>\n",
       "      <td>0.638364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_iter ngram_range  min_df     C penalty multi_class  fit_itercept  \\\n",
       "153     10000      (1, 2)      10  0.01      l2         ovr          True   \n",
       "155     10000      (1, 2)      10  0.01      l2         ovr         False   \n",
       "105     10000      (1, 2)       5  0.01      l2         ovr          True   \n",
       "107     10000      (1, 2)       5  0.01      l2         ovr         False   \n",
       "11      10000      (1, 1)       5  0.01      l2         ovr         False   \n",
       "9       10000      (1, 1)       5  0.01      l2         ovr          True   \n",
       "57      10000      (1, 1)      10  0.01      l2         ovr          True   \n",
       "59      10000      (1, 1)      10  0.01      l2         ovr         False   \n",
       "169     10000      (1, 2)      10  0.10      l2         ovr          True   \n",
       "171     10000      (1, 2)      10  0.10      l2         ovr         False   \n",
       "\n",
       "    class_weight  accuracy        f1  \n",
       "153     balanced    0.6695  0.662028  \n",
       "155     balanced    0.6695  0.662028  \n",
       "105     balanced    0.6680  0.659995  \n",
       "107     balanced    0.6680  0.659995  \n",
       "11      balanced    0.6620  0.652575  \n",
       "9       balanced    0.6620  0.652575  \n",
       "57      balanced    0.6620  0.652007  \n",
       "59      balanced    0.6620  0.652007  \n",
       "169     balanced    0.6455  0.638364  \n",
       "171     balanced    0.6455  0.638364  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_svc_c).dropna().sort_values(\"f1\",ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tfidf Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'max_iter': 10000, 'ngram_range': (2, 2), 'min_df': 10, 'C': 1, 'penalty': 'l2', 'multi_class': 'crammer', 'fit_itercept': False, 'class_weight': 'balanced', 'accuracy': nan, 'f1': nan}, combination 288 out of 28884 out of 2882888"
     ]
    }
   ],
   "source": [
    "res_svc_t, pipe_svc_t = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = LinearSVC, vectorizer = TfidfVectorizer, \n",
    "                      param_space = param_space, return_best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_iter</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>multi_class</th>\n",
       "      <th>fit_itercept</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6655</td>\n",
       "      <td>0.660108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6655</td>\n",
       "      <td>0.660108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>0.652416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>0.652416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6545</td>\n",
       "      <td>0.647857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6545</td>\n",
       "      <td>0.647857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6480</td>\n",
       "      <td>0.641912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6480</td>\n",
       "      <td>0.641912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>False</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>0.631038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>10000</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>l2</td>\n",
       "      <td>ovr</td>\n",
       "      <td>True</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.6360</td>\n",
       "      <td>0.631038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_iter ngram_range  min_df    C penalty multi_class  fit_itercept  \\\n",
       "169     10000      (1, 2)      10  0.1      l2         ovr          True   \n",
       "171     10000      (1, 2)      10  0.1      l2         ovr         False   \n",
       "123     10000      (1, 2)       5  0.1      l2         ovr         False   \n",
       "121     10000      (1, 2)       5  0.1      l2         ovr          True   \n",
       "73      10000      (1, 1)      10  0.1      l2         ovr          True   \n",
       "75      10000      (1, 1)      10  0.1      l2         ovr         False   \n",
       "27      10000      (1, 1)       5  0.1      l2         ovr         False   \n",
       "25      10000      (1, 1)       5  0.1      l2         ovr          True   \n",
       "187     10000      (1, 2)      10  1.0      l2         ovr         False   \n",
       "185     10000      (1, 2)      10  1.0      l2         ovr          True   \n",
       "\n",
       "    class_weight  accuracy        f1  \n",
       "169     balanced    0.6655  0.660108  \n",
       "171     balanced    0.6655  0.660108  \n",
       "123     balanced    0.6580  0.652416  \n",
       "121     balanced    0.6580  0.652416  \n",
       "73      balanced    0.6545  0.647857  \n",
       "75      balanced    0.6545  0.647857  \n",
       "27      balanced    0.6480  0.641912  \n",
       "25      balanced    0.6480  0.641912  \n",
       "187     balanced    0.6360  0.631038  \n",
       "185     balanced    0.6360  0.631038  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_svc_t).dropna().sort_values(\"f1\",ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: KNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Count Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\"n_neighbors\":[3, 5, 9],\n",
    "              \"weights\":[\"uniform\",\"distance\"],\n",
    "              \"p\":[1,2,3],\n",
    "               \"n_jobs\":[-1],\n",
    "              \"ngram_range\":[(1,1),(1,2),(2,2)],\n",
    "              \"min_df\":[5, 10, 50]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'n_neighbors': 9, 'weights': 'distance', 'p': 3, 'n_jobs': -1, 'ngram_range': (2, 2), 'min_df': 50, 'accuracy': nan, 'f1': nan}, combination 162 out of 162tion 153 out of 162"
     ]
    }
   ],
   "source": [
    "res_knn_c, pipe_knn_c = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = KNeighborsClassifier, vectorizer = CountVectorizer, \n",
    "                      param_space = param_space, return_best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>p</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.446163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4905</td>\n",
       "      <td>0.442282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>0.439183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4915</td>\n",
       "      <td>0.436836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4725</td>\n",
       "      <td>0.435407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5090</td>\n",
       "      <td>0.434364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4980</td>\n",
       "      <td>0.434326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>0.433873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4710</td>\n",
       "      <td>0.433472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4560</td>\n",
       "      <td>0.431850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_neighbors   weights  p  n_jobs ngram_range  min_df  accuracy        f1\n",
       "96             5  distance  2      -1      (2, 2)       5    0.4905  0.446163\n",
       "69             5   uniform  2      -1      (2, 2)       5    0.4905  0.442282\n",
       "142            9  distance  1      -1      (2, 2)      10    0.4930  0.439183\n",
       "151            9  distance  2      -1      (2, 2)      10    0.4915  0.436836\n",
       "33             3  distance  1      -1      (2, 2)       5    0.4725  0.435407\n",
       "92             5  distance  2      -1      (1, 1)      50    0.5090  0.434364\n",
       "116            9   uniform  1      -1      (2, 2)      50    0.4980  0.434326\n",
       "38             3  distance  2      -1      (1, 1)      50    0.4910  0.433873\n",
       "42             3  distance  2      -1      (2, 2)       5    0.4710  0.433472\n",
       "141            9  distance  1      -1      (2, 2)       5    0.4560  0.431850"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_knn_c).dropna().sort_values(\"f1\",ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tfidf Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated parameters {'n_neighbors': 9, 'weights': 'distance', 'p': 3, 'n_jobs': -1, 'ngram_range': (2, 2), 'min_df': 50, 'accuracy': nan, 'f1': nan}, combination 162 out of 162on 153 out of 16222"
     ]
    }
   ],
   "source": [
    "res_knn_t, pipe_knn_t = grid_search_dev(train, ytrain, dev, ydev,\n",
    "                      classifier = KNeighborsClassifier, vectorizer = TfidfVectorizer, \n",
    "                      param_space = param_space, return_best = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>weights</th>\n",
       "      <th>p</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>min_df</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.461022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.453816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.452250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5035</td>\n",
       "      <td>0.452043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4955</td>\n",
       "      <td>0.451365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5075</td>\n",
       "      <td>0.450785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>0.450235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>0.443815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>50</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.438791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>0.438753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n_neighbors   weights  p  n_jobs ngram_range  min_df  accuracy        f1\n",
       "88             5  distance  1      -1      (2, 2)      10    0.5040  0.461022\n",
       "38             3  distance  2      -1      (1, 1)      50    0.4890  0.453816\n",
       "115            9   uniform  1      -1      (2, 2)      10    0.5040  0.452250\n",
       "142            9  distance  1      -1      (2, 2)      10    0.5035  0.452043\n",
       "61             5   uniform  1      -1      (2, 2)      10    0.4955  0.451365\n",
       "124            9   uniform  2      -1      (2, 2)      10    0.5075  0.450785\n",
       "151            9  distance  2      -1      (2, 2)      10    0.5060  0.450235\n",
       "97             5  distance  2      -1      (2, 2)      10    0.4930  0.443815\n",
       "92             5  distance  2      -1      (1, 1)      50    0.4975  0.438791\n",
       "96             5  distance  2      -1      (2, 2)       5    0.4640  0.438753"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_knn_t).dropna().sort_values(\"f1\",ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Bag-of-words and traditional approaches:\n",
    "To summarize the results, it appears that Support Vector Machines outperform all models in terms of both macro-averaged F1 score and accuracy. What is more suprising is that the Tfidf transformation of the bag-of-words feature matrix does not offer much improvement in performance of either model in comparison to the similer count vectorization. In fact, some models, such as Logistic Regression or Naive Bayes classifier seem to outperform their Tfidf-based counterparts when using count vectorization. This paradoxical finding might result from the fact that more frequent tokens in the corpus carry more predictive power (i.e. sentimental meaning) than rare ones - Tfidf scores for such tokens are likely to be close to 0, which would limit their impact on the model's decision boundaries. This makes intuitive sense, as words we use to express our emotions are likely to be more frequent and come from a vocabularity set with relatively low cardiality, as compared to multitude of descriptive tokens with limited valence. The table below provides summaries of the re-fitted modes on three previously unseen datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NaiveBayes-Count</th>\n",
       "      <th>MaxEnt-Count</th>\n",
       "      <th>SVC-Count</th>\n",
       "      <th>KNN-Tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test1</th>\n",
       "      <td>0.571585</td>\n",
       "      <td>0.554163</td>\n",
       "      <td>0.630536</td>\n",
       "      <td>0.387227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test2</th>\n",
       "      <td>0.568195</td>\n",
       "      <td>0.575681</td>\n",
       "      <td>0.622686</td>\n",
       "      <td>0.381052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test3</th>\n",
       "      <td>0.554254</td>\n",
       "      <td>0.544365</td>\n",
       "      <td>0.590027</td>\n",
       "      <td>0.379937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       NaiveBayes-Count  MaxEnt-Count  SVC-Count  KNN-Tfidf\n",
       "test1          0.571585      0.554163   0.630536   0.387227\n",
       "test2          0.568195      0.575681   0.622686   0.381052\n",
       "test3          0.554254      0.544365   0.590027   0.379937"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = defaultdict(lambda: dict())\n",
    "for i, (data, target) in enumerate(zip([test1, test2, test3], [ytest1, ytest2, ytest3])):\n",
    "    for j, model in enumerate([pipe_nb_c, pipe_logit_c, pipe_svc_c, pipe_knn_t]):\n",
    "        ypred = model.predict(data)\n",
    "        res[models[j]][\"test\" + str(i + 1)] = f1_score(ypred, target, average = \"macro\") \n",
    "pd.DataFrame(dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Neural Networks\n",
    "The superiority of neural network models in Natural Language Processing has been recognized for a while. The second part of the project evaluated whether such models outperform traditional ML approaches on the 2017 sentiment evaluation task with a relatively small number of training examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe and LSTM networks - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing in this case is slightly different. The data is represented as integer sequences and then matched with the appropriate GloVe embedding vectors. The GloVe vectors were obtained from the pre-trained embeddings database shared by the Stanford University, available here https://nlp.stanford.edu/projects/glove/. Due to high dimensionality of the data, only the top fraction of the most popular tokens are used in the task.  Furthermore, special tokens (emoticons, etc) are not used in the task, as they don't have a matching glove vector representation. et sequence length to max length of training tweet and maximum words to be used to 5000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, obtain one-hot encoded targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain, ydev, ytest1, ytest2, ytest3 = [pd.get_dummies(target).values for target in \n",
    "                                        [ytrain, ydev, ytest1, ytest2, ytest3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lstm(tweets):\n",
    "    print(f\"Vocabulary size pre-preprocessing {len(set(chain.from_iterable([tweet.split() for tweet in tweets])))}\")\n",
    "    \n",
    "    tweets = [tweet.lower() for tweet in tweets] #lowercase\n",
    "    \n",
    "    #remove urls\n",
    "    tweets = [re.sub(r\"(?:https?\\:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.(?:(?:[a-zA-Z0-9-]+\\.)*)?[a-z]{2,4}(?:(?:(\\/|\\?)\\S+)*)?\",' ',tweet) for tweet in tweets]\n",
    "\n",
    "    #remove emojis and emoticons\n",
    "    tweets = [re.sub(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])',' ',tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r\"[oO>]?[;:=Xx8]+'?\\-?[)>)pPdD3\\]\\}\\*]+\",' ',tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r\"[oO>]?[:;=]+'?\\-?[(<\\\\\\/\\[)\\{]+\",' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove words of length == 1\n",
    "    tweets = [re.sub(r'\\b(\\w)\\b',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove numbers\n",
    "    tweets = [re.sub(r\"(\\S+)?\\d+(\\S+)?\",' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove hashtags\n",
    "    tweets = [re.sub(r'\\#[a-zA-Z0-9]+',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove user mentions\n",
    "    tweets = [re.sub(r'\\@\\w+(?!\\.\\w+)\\b',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove non alphanumeric\n",
    "    tweets = [re.sub(r\"\\&amp\",\" \",tweet) for tweet in tweets] #bug in twitter - &amp appearing\n",
    "    tweets = [re.sub(r\"[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]+\",\" \",tweet) for tweet in tweets]\n",
    "    \n",
    "    #replace underscores with space\n",
    "    tweets = [re.sub(r\"_+\",\" \",tweet) for tweet in tweets]\n",
    "    \n",
    "    \n",
    "    #remove extra whitespaces:\n",
    "    tweets = [re.sub(r' +',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    \n",
    "    #tokenize to remove stopwords:\n",
    "    tweets = [word_tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = {word for word in stopwords.words('english') if not re.compile(r\"\\b(\\w+nt|no|not)\\b\").match(word)}\n",
    "    for i in trange(len(tweets)):\n",
    "        tweets[i] = [word for word in tweets[i] if word not in stop_words]\n",
    "    \n",
    "    #lemmatize and join\n",
    "    tweets = lemmatize(tweets)\n",
    "    \n",
    "    print(f\"Vocabulary size post-preprocessing {len(set(chain.from_iterable([tweet.split() for tweet in tweets])))}\")\n",
    "    \n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size pre-preprocessing 131482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45026/45026 [00:00<00:00, 416732.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size post-preprocessing 30504\n",
      "Vocabulary size pre-preprocessing 12908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 433452.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size post-preprocessing 5504\n",
      "Vocabulary size pre-preprocessing 18855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3531/3531 [00:00<00:00, 363553.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size post-preprocessing 7435\n",
      "Vocabulary size pre-preprocessing 10424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1853/1853 [00:00<00:00, 217692.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size post-preprocessing 4257\n",
      "Vocabulary size pre-preprocessing 12401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2379/2379 [00:00<00:00, 403145.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size post-preprocessing 5171\n"
     ]
    }
   ],
   "source": [
    "train, dev, test1, test2, test3 = list(map(lambda x: preprocess_lstm(x[\"text\"].tolist()), \n",
    "                                           [twitter_train, twitter_dev, twitter_test1, twitter_test2, twitter_test3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing pre-processing, I tokenize and integer encode the data, keeping only top 10000 most popular words, with overall training vocabulary size of ~30000 after preprocessing and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000 #number of words to be used in the model\n",
    "#init tokenizer and fit on training data:\n",
    "twitter_tokenizer = Tokenizer(num_words = max_words, split = ' ',filters = ' ')\n",
    "twitter_tokenizer.fit_on_texts(train)\n",
    "\n",
    "#tokenize\n",
    "train, dev, test1, test2, test3 = [twitter_tokenizer.texts_to_sequences(data) for data in \n",
    "                                   [train, dev, test1, test2, test3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the length of sequences, I evaluate the distribution of sequence lengths in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45026.0</td>\n",
       "      <td>9.481055</td>\n",
       "      <td>3.399122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count      mean       std  min  25%   50%   75%    max\n",
       "0  45026.0  9.481055  3.399122  0.0  7.0  10.0  12.0  130.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(x) for x in train]\n",
    "plt.hist(lengths, bins = 100)\n",
    "plt.show()\n",
    "pd.DataFrame(lengths).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the distribution above, padding the sequences to the length of 20 seems like a reasonable idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad\n",
    "seq_len = 20\n",
    "train, dev, test1, test2, test3 = [pad_sequences(data, seq_len) for data in \n",
    "                                   [train, dev, test1, test2, test3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained GloVe embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9715\n"
     ]
    }
   ],
   "source": [
    "emb_mat = np.zeros((max_words, 100))\n",
    "with open(\"/home/piotr/nlp/glove.6B.100d.txt\",encoding = \"utf-8\") as f:\n",
    "    for line in f:\n",
    "        elements = line.split()\n",
    "        if elements[0] in twitter_tokenizer.word_index and twitter_tokenizer.word_index[elements[0]] < max_words:\n",
    "            emb_mat[twitter_tokenizer.word_index[elements[0]]] = elements[1:]\n",
    "print(np.sum(emb_mat.any(axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Pytorch utilities and define helper functions to convert between numpy arrays and torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "#pass to appropriate device\n",
    "USE_CUDA = torch.cuda.is_available() \n",
    "def cuda(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n",
    "\n",
    "#convert numpy to tensor\n",
    "def toTensor(v,dtype = torch.float,requires_grad = False):       \n",
    "    return cuda(Variable(torch.tensor(v)).type(dtype).requires_grad_(requires_grad))\n",
    "\n",
    "#convert tensor to numpy\n",
    "def toNumpy(v):\n",
    "    if USE_CUDA:\n",
    "        return v.detach().cpu().numpy()\n",
    "    return v.detach().numpy()\n",
    "\n",
    "#convert to torch dataset\n",
    "def toTorchData(X, Y):\n",
    "    x = toTensor(X, dtype = torch.long)\n",
    "    y = toTensor(Y, dtype = torch.long)\n",
    "    dataset = torch.utils.data.TensorDataset(x, y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert everything into pytorch tensors & datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test1, test2, test3 = [toTorchData(x, y) for x, y in \n",
    " zip([train, dev, test1, test2, test3],\n",
    "     [ytrain, ydev, ytest1, ytest2, ytest3])]\n",
    "emb_mat = toTensor(emb_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network evaluation\n",
    "I defined a function to create and write clear-cut evaluation summaries as Python dictionaries, serialize them intro json strings and write them to a text file. This allows me to keep track of the subsequent training results and corresponding hyperparameter combinations to ensure that I'm moving in the right direction while optimizing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(x, ytrue, model, optimizer, num_epochs, file = None):\n",
    "    \"\"\"\n",
    "    Evaluate network results using pairwise F1 scores and Accuracy. \n",
    "    Write results to file if specified.\n",
    "    x: torch.Tensor\n",
    "        test data\n",
    "    y: np.array\n",
    "        one-hot encoded targets\n",
    "    model: torch.nn.Module\n",
    "        fitted network\n",
    "    file: str\n",
    "        filename to save parameters\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ypred = model(x).detach().numpy().argmax(axis = 1)\n",
    "    \n",
    "    \n",
    "    results = dict()\n",
    "    \n",
    "    #get params:\n",
    "    for name, param in model.named_parameters():\n",
    "        results[name] = param.shape[0]\n",
    "    \n",
    "    #add optimizer parameters\n",
    "    results.update(optimizer.defaults)\n",
    "    \n",
    "    #add num epochs\n",
    "    results[\"num_epochs\"] = num_epochs\n",
    "    \n",
    "    #get overall and pairwise F1:\n",
    "    results[\"f1_overall\"] = f1_score(ytrue, ypred, average = \"macro\")\n",
    "    results[\"f1_neg_vs_neu\"] = f1_score(ytrue, ypred, average = \"macro\", labels = [0,1])\n",
    "    results[\"f1_neu_vs_pos\"] = f1_score(ytrue, ypred, average = \"macro\", labels = [1,2])\n",
    "    results[\"f1_neg_vs_pos\"] = f1_score(ytrue, ypred, average = \"macro\", labels = [0,2])\n",
    "    #get accuracy:\n",
    "    results[\"acc\"] = accuracy_score(ytrue, ypred)\n",
    "    \n",
    "    print(confusion_matrix(ytrue, ypred))\n",
    "    \n",
    "    #convert to string to make it json-serializable\n",
    "    results = {k:str(v) for k, v in results.items()}\n",
    "    \n",
    "    if file and os.path.isfile(file):\n",
    "        with open(file, \"a\") as f:\n",
    "            line = json.dumps(results)\n",
    "            f.write(line + \"\\n\")\n",
    "    elif file and not os.path.isfile(file):\n",
    "        with open(file, \"w\") as f:\n",
    "            line = json.dumps(results)\n",
    "            f.write(line + \"\\n\")\n",
    "            \n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 1 - Unidirectional LSTM \n",
    "**Using 100 dimensional GloVe embeddings for the top 10000 words in the training corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        #match embeddings with pretrained - frozen layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_mat)\n",
    "        #batch normalization - regularize:\n",
    "        self.norm = nn.SyncBatchNorm(100)\n",
    "        #LSTM - 128 units of hidden size\n",
    "        self.lstm = nn.LSTM(input_size = 100, hidden_size = 256, batch_first = True)\n",
    "        #linear classifier\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(256, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.norm(out)\n",
    "        out, (_, _) = self.lstm(out)\n",
    "        #out = self.norm(out[:,-1,:].view(-1, 256)) #get last hidden state and reshape\n",
    "        out = self.clf(out)\n",
    "        out = F.softmax(out.view(-1, 3), dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I train the network with specified number of epochs, batch size and learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "model = cuda(LSTM())\n",
    "\n",
    "#loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle = True)\n",
    "#loader for validation\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(dataset = dev, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MultiLabelSoftMarginLoss(weight = torch.tensor([2, 1, 1]))  #Cross entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.001)\n",
    "history = {\"train\":[], \"validation\":[]}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        #training\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for inputs, labels in train_loader:  #pick a batch\n",
    "            # Pass to cuda\n",
    "            inputs = cuda(inputs)\n",
    "            labels = cuda(labels)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs) #forward pass\n",
    "            loss = criterion(outputs, labels) #loss\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            loss.backward() #backward pass\n",
    "            optimizer.step() #optimization\n",
    "            train_loss.append(toNumpy(loss)) #save loss\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\rEpoch {} Mean training loss {}\".format(epoch, np.mean(train_loss)), end = \"\")\n",
    "        history[\"train\"].append(np.mean(train_loss))\n",
    "            \n",
    "            \n",
    "        #evaluation\n",
    "        model.eval()\n",
    "        validation_loss = []\n",
    "        for inputs, labels in dev_loader:\n",
    "            # Pass to cuda\n",
    "            inputs = cuda(inputs)\n",
    "            labels = cuda(labels)\n",
    "            outputs = model(inputs) #get outputs\n",
    "            loss = criterion(outputs, labels) #compute loss\n",
    "            validation_loss.append(toNumpy(loss)) #save loss\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\rEpoch {} Mean validation loss {}\".format(epoch, np.mean(validation_loss)), end = \"\")\n",
    "        history[\"validation\"].append(np.mean(validation_loss))\n",
    "#perform evaluation and append to log file\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(history[\"train\"])\n",
    "ax.plot(history[\"validation\"])\n",
    "ax.legend([\"training\",\"validation\"])\n",
    "plt.show()\n",
    "print(evaluate_network(dev.tensors[0], ydev.argmax(axis = 1), model, optimizer, num_epochs, \"lstm_results.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the results of runs with different hyperparameter combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding.weight</th>\n",
       "      <th>lstm.weight_ih_l0</th>\n",
       "      <th>lstm.weight_hh_l0</th>\n",
       "      <th>lstm.bias_ih_l0</th>\n",
       "      <th>lstm.bias_hh_l0</th>\n",
       "      <th>norm.weight</th>\n",
       "      <th>norm.bias</th>\n",
       "      <th>clf.0.weight</th>\n",
       "      <th>clf.0.bias</th>\n",
       "      <th>clf.2.weight</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_neg_vs_pos</th>\n",
       "      <th>acc</th>\n",
       "      <th>clf.3.weight</th>\n",
       "      <th>clf.3.bias</th>\n",
       "      <th>clf.5.weight</th>\n",
       "      <th>clf.5.bias</th>\n",
       "      <th>lstm.weight_ih_l1</th>\n",
       "      <th>lstm.weight_hh_l1</th>\n",
       "      <th>lstm.bias_ih_l1</th>\n",
       "      <th>lstm.bias_hh_l1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6123986045780379</td>\n",
       "      <td>0.647</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6123416120756193</td>\n",
       "      <td>0.661</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10000</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6080335161750097</td>\n",
       "      <td>0.6495</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6075974944433218</td>\n",
       "      <td>0.6595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6049740053624868</td>\n",
       "      <td>0.657</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6032423863521157</td>\n",
       "      <td>0.6435</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5977534383185965</td>\n",
       "      <td>0.6495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10000</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5966530612244898</td>\n",
       "      <td>0.6445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10000</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5957502151796848</td>\n",
       "      <td>0.6465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10000</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5939242089923631</td>\n",
       "      <td>0.648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   embedding.weight lstm.weight_ih_l0 lstm.weight_hh_l0 lstm.bias_ih_l0  \\\n",
       "9             10000              2048              2048            2048   \n",
       "7             10000              1024              1024            1024   \n",
       "10            10000              2048              2048            2048   \n",
       "3             10000               512               512             512   \n",
       "15            10000              1024              1024            1024   \n",
       "8             10000              1024              1024            1024   \n",
       "16            10000              1024              1024            1024   \n",
       "14            10000              1024              1024            1024   \n",
       "11            10000               512               512             512   \n",
       "12            10000               512               512             512   \n",
       "\n",
       "   lstm.bias_hh_l0 norm.weight norm.bias clf.0.weight clf.0.bias clf.2.weight  \\\n",
       "9             2048         512       512          256        256          NaN   \n",
       "7             1024         256       256          128        128          NaN   \n",
       "10            2048         512       512          256        256          NaN   \n",
       "3              512         128       128          128        128            3   \n",
       "15            1024         256       256          256        256          NaN   \n",
       "8             1024         256       256          128        128          NaN   \n",
       "16            1024         256       256            3          3          NaN   \n",
       "14            1024         256       256            3          3          NaN   \n",
       "11             512         NaN       NaN            3          3          NaN   \n",
       "12             512         NaN       NaN            3          3          NaN   \n",
       "\n",
       "    ...       f1_neg_vs_pos     acc clf.3.weight clf.3.bias clf.5.weight  \\\n",
       "9   ...  0.6123986045780379   0.647            3          3          NaN   \n",
       "7   ...  0.6123416120756193   0.661          128        128            3   \n",
       "10  ...  0.6080335161750097  0.6495            3          3          NaN   \n",
       "3   ...  0.6075974944433218  0.6595          NaN        NaN          NaN   \n",
       "15  ...  0.6049740053624868   0.657            3          3          NaN   \n",
       "8   ...  0.6032423863521157  0.6435          128        128            3   \n",
       "16  ...  0.5977534383185965  0.6495          NaN        NaN          NaN   \n",
       "14  ...  0.5966530612244898  0.6445          NaN        NaN          NaN   \n",
       "11  ...  0.5957502151796848  0.6465          NaN        NaN          NaN   \n",
       "12  ...  0.5939242089923631   0.648          NaN        NaN          NaN   \n",
       "\n",
       "   clf.5.bias lstm.weight_ih_l1 lstm.weight_hh_l1 lstm.bias_ih_l1  \\\n",
       "9         NaN               NaN               NaN             NaN   \n",
       "7           3               NaN               NaN             NaN   \n",
       "10        NaN               NaN               NaN             NaN   \n",
       "3         NaN               NaN               NaN             NaN   \n",
       "15        NaN              1024              1024            1024   \n",
       "8           3               NaN               NaN             NaN   \n",
       "16        NaN               NaN               NaN             NaN   \n",
       "14        NaN              1024              1024            1024   \n",
       "11        NaN               NaN               NaN             NaN   \n",
       "12        NaN               512               512             512   \n",
       "\n",
       "   lstm.bias_hh_l1  \n",
       "9              NaN  \n",
       "7              NaN  \n",
       "10             NaN  \n",
       "3              NaN  \n",
       "15            1024  \n",
       "8              NaN  \n",
       "16             NaN  \n",
       "14            1024  \n",
       "11             NaN  \n",
       "12             512  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "with open(\"lstm_results.txt\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        res.append(json.loads(line))\n",
    "pd.DataFrame(res).sort_values(\"f1_neg_vs_pos\", ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 2 - Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMBi, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_mat, padding_idx = 0)\n",
    "        #LSTM - 50 units of hidden size\n",
    "        self.lstm = nn.LSTM(input_size = 100, hidden_size = 128,\n",
    "                            batch_first = False, bidirectional = True)\n",
    "        #batch normalization - regularize:\n",
    "        self.norm = nn.BatchNorm1d(256)\n",
    "        #linear classifier\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, (_, _) = self.lstm(out)\n",
    "        #concatenate last hidden state of the normal LSTM and \"first\" neuron of the reverse LSTMs\n",
    "        out = torch.cat((out[:, -1, :128], out[:, 0, 128:]), dim = 1) \n",
    "        out = self.norm(out.view(-1, 256))\n",
    "        out = self.clf(out)\n",
    "        out = F.softmax(out.view(-1, 3), dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(emb_mat, padding_idx = 0)\n",
    "lstm = nn.LSTM(input_size = 100, hidden_size = 128, batch_first = True)\n",
    "out, (h0, c0) = lstm(embedding(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "model = cuda(LSTMBi())\n",
    "\n",
    "#loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "#loader for validation\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  #Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "history = {\"train\":[], \"validation\":[]}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        #training\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for inputs, labels in train_loader:  #pick a batch\n",
    "            # Pass to cuda\n",
    "            inputs = cuda(inputs)\n",
    "            labels = cuda(labels)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs) #forward pass\n",
    "            loss = criterion(outputs, labels) #loss\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            loss.backward() #backward pass\n",
    "            optimizer.step() #optimization\n",
    "            train_loss.append(toNumpy(loss)) #save loss\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\rEpoch {} Mean training loss {}\".format(epoch, np.mean(train_loss)), end = \"\")\n",
    "        history[\"train\"].append(np.mean(train_loss))\n",
    "            \n",
    "            \n",
    "        #evaluation\n",
    "        model.eval()\n",
    "        validation_loss = []\n",
    "        for inputs, labels in dev_loader:\n",
    "            # Pass to cuda\n",
    "            inputs = cuda(inputs)\n",
    "            labels = cuda(labels)\n",
    "            outputs = model(inputs) #get outputs\n",
    "            loss = criterion(outputs, labels) #compute loss\n",
    "            validation_loss.append(toNumpy(loss)) #save loss\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\rEpoch {} Mean validation loss {}\".format(epoch, np.mean(validation_loss)), end = \"\")\n",
    "        history[\"validation\"].append(np.mean(validation_loss))\n",
    "#perform evaluation and append to log file\n",
    "print(evaluate_network(dev.tensors[0], ydev, model, optimizer, num_epochs, \"bilstm_results.txt\"))\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(history[\"train\"])\n",
    "ax.plot(history[\"validation\"])\n",
    "ax.legend([\"training\",\"validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network 3 -  BERT Transformer\n",
    "In the final step, I used Google's pre-trained BERT transformer, which is considered the current state-of-the-art in the Natural Language Processing domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing steps are basically the same as for LSTM, but this time they use `BertTokenizerFast` for tokenization and also combine the feature matrix with a mask covering 0 paddings from the self-attention algorithm and the targets into torch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_bert(tweets, targets, max_length):\n",
    "    print(f\"Vocabulary size pre-preprocessing {len(set(chain.from_iterable([tweet.split() for tweet in tweets])))}\")\n",
    "    \n",
    "    tweets = [tweet.lower() for tweet in tweets] #lowercase\n",
    "    \n",
    "    #remove urls\n",
    "    tweets = [re.sub(r\"(?:https?\\:\\/\\/)?(?:www\\.)?[a-zA-Z0-9-]+\\.(?:(?:[a-zA-Z0-9-]+\\.)*)?[a-z]{2,4}(?:(?:(\\/|\\?)\\S+)*)?\",' ',tweet) for tweet in tweets]\n",
    "\n",
    "    #remove emojis and emoticons\n",
    "    tweets = [re.sub(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])',' ',tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r\"[oO>]?[;:=Xx8]+'?\\-?[)>)pPdD3\\]\\}\\*]+\",' ',tweet) for tweet in tweets]\n",
    "    tweets = [re.sub(r\"[oO>]?[:;=]+'?\\-?[(<\\\\\\/\\[)\\{]+\",' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove words of length == 1\n",
    "    tweets = [re.sub(r'\\b(\\w)\\b',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove numbers\n",
    "    tweets = [re.sub(r\"(\\S+)?\\d+(\\S+)?\",' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove hashtags\n",
    "    tweets = [re.sub(r'\\#[a-zA-Z0-9]+',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove user mentions\n",
    "    tweets = [re.sub(r'\\@\\w+(?!\\.\\w+)\\b',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove non alphanumeric\n",
    "    tweets = [re.sub(r\"\\&amp\",\" \",tweet) for tweet in tweets] #bug in twitter - &amp appearing\n",
    "    tweets = [re.sub(r\"[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]+\",\" \",tweet) for tweet in tweets]\n",
    "    \n",
    "    #replace underscores with space\n",
    "    tweets = [re.sub(r\"_+\",\" \",tweet) for tweet in tweets]\n",
    "    \n",
    "    \n",
    "    #remove extra whitespaces:\n",
    "    tweets = [re.sub(r' +',' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    \n",
    "    #tokenize to remove stopwords:\n",
    "    tweets = [word_tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = {word for word in stopwords.words('english') if not re.compile(r\"\\b(\\w+nt|no|not)\\b\").match(word)}\n",
    "    for i in trange(len(tweets)):\n",
    "        tweets[i] = [word for word in tweets[i] if word not in stop_words]\n",
    "    \n",
    "    #lemmatize and join\n",
    "    tweets = lemmatize(tweets)\n",
    "    \n",
    "    print(f\"Vocabulary size post-preprocessing {len(set(chain.from_iterable([tweet.split() for tweet in tweets])))}\")\n",
    "    \n",
    "    \n",
    "    #preform tokenization\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    tweets = [tokenizer.encode(tweet, pad_to_max_length = True, max_length = max_length) for tweet in tweets]\n",
    "    \n",
    "    tweets =  toTensor(tweets, dtype = torch.long)\n",
    "    targets = toTensor(targets)\n",
    "    mask = (tweets != 0).long()\n",
    "    dataset = torch.utils.data.TensorDataset(tweets, mask, targets)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test1, test2, test3 = [preprocess_bert(x[\"text\"].tolist(), y) for x, y in \n",
    "                                   zip([twitter_train, twitter_dev, twitter_test1, twitter_test2, twitter_test3],\n",
    "                                       [ytrain, ydev, ytest1, ytest2, ytest3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.linear = nn.Linear(768, 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        out, _ = self.bert(x, attention_mask = attn_mask)\n",
    "        out = out[:,0] #cls pooling\n",
    "        out = self.linear(out)\n",
    "        out = F.softmax(out, dim = 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "model = cuda(Bert())\n",
    "\n",
    "#loader for training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "#loader for validation\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MultiLabelSoftMarginLoss()  #Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "history = {\"train\":[], \"validation\":[]}\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        #training\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for inputs, mask, labels in train_loader:  #pick a batch\n",
    "            # Pass to cuda\n",
    "            inputs = cuda(inputs)\n",
    "            labels = cuda(labels)\n",
    "\n",
    "            \n",
    "            outputs = model(inputs, mask) #forward pass\n",
    "            loss = criterion(outputs, labels) #loss\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            loss.backward() #backward pass\n",
    "            optimizer.step() #optimization\n",
    "            train_loss.append(toNumpy(loss)) #save loss\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\rEpoch {} Mean training loss {}\".format(epoch, np.mean(train_loss)), end = \"\")\n",
    "        history[\"train\"].append(np.mean(train_loss))\n",
    "            \n",
    "            \n",
    "        #evaluation\n",
    "        model.eval()\n",
    "        validation_loss = []\n",
    "        for inputs, mask, labels in dev_loader:\n",
    "            # Pass to cuda\n",
    "            inputs = cuda(inputs)\n",
    "            labels = cuda(labels)\n",
    "            outputs = model(inputs, mask) #get outputs\n",
    "            loss = criterion(outputs, labels) #compute loss\n",
    "            validation_loss.append(toNumpy(loss)) #save loss\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\rEpoch {} Mean validation loss {}\".format(epoch, np.mean(validation_loss)), end = \"\")\n",
    "        history[\"validation\"].append(np.mean(validation_loss))\n",
    "#perform evaluation and append to log file\n",
    "evaluate_network(dev.tensors[0], ydev, model, optimizer, num_epochs, \"bert_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
